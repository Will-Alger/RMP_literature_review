
@inproceedings{gordon_role_2021,
	location = {New York, {NY}, {USA}},
	title = {The Role of Race and Gender in Teaching Evaluation of Computer Science Professors: A Large Scale Analysis on {RateMyProfessor} Data},
	isbn = {978-1-4503-8062-1},
	url = {https://doi.org/10.1145/3408877.3432369},
	doi = {10.1145/3408877.3432369},
	series = {{SIGCSE} '21},
	shorttitle = {The Role of Race and Gender in Teaching Evaluation of Computer Science Professors},
	abstract = {Recently, Computer Science ({CS}) education has experienced a renewed interest, driven by the demand in the fast-changing job market. This renewed interest created an uptick of enrollment in computer science courses. Increased number of students search for information about {CS} courses and professors. Often times, students turn to a professor's profile on online sites, e.g. {RateMyProfessor}.com ({RMP}), to read feedback and assessments made by other students. Student Evaluations of Teaching ({SETs}), conducted online or on paper, are widely used to assess and improve the teaching quality of professors, and to provide critical assessment of the teaching material and content. This paper studies the role of race and gender of computer science professors on their teaching evaluation by analyzing the publicly available data of over 39,000 {CS} professors on {RateMyProfessor}. We found that women are generally rated lower then men in overall teaching quality. They are also perceived lower in personality-related student feedback ratings, i.e. they perceived less humorous, and less inspirational. We also found that Asian professors are perceived to be tough graders and lecture heavy. They are also perceived to be more difficult in general.},
	pages = {980--986},
	booktitle = {Proceedings of the 52nd {ACM} Technical Symposium on Computer Science Education},
	publisher = {Association for Computing Machinery},
	author = {Gordon, Nikolas and Alam, Omar},
	urldate = {2023-09-21},
	date = {2021-03-05},
	keywords = {computer science education, gender, race, {RateMyProfessor}, student evaluation of teaching},
	file = {Gordon and Alam - 2021 - The Role of Race and Gender in Teaching Evaluation.pdf:C\:\\Users\\Will\\Zotero\\storage\\QNIEA96R\\Gordon and Alam - 2021 - The Role of Race and Gender in Teaching Evaluation.pdf:application/pdf},
}

@article{rosen_correlations_2018,
	title = {Correlations, trends and potential biases among publicly accessible web-based student evaluations of teaching: a large-scale study of {RateMyProfessors}.com data},
	volume = {43},
	issn = {0260-2938},
	url = {https://doi.org/10.1080/02602938.2016.1276155},
	doi = {10.1080/02602938.2016.1276155},
	shorttitle = {Correlations, trends and potential biases among publicly accessible web-based student evaluations of teaching},
	abstract = {Student evaluations of teaching are widely adopted across academic institutions, but there are many underlying trends and biases that can influence their interpretation. Publicly accessible web-based student evaluations of teaching are of particular relevance, due to their widespread use by students in the course selection process and the quantity of data available for analysis. In this study, data from the most popular of these websites, {RateMyProfessors}.com, is analysed for correlations between measures of instruction quality, easiness, physical attractiveness, discipline and gender. This study of 7,882,980 {RateMyProfessors} ratings (from 190,006 {US} professors with at least 20 student ratings) provides further insight into student perceptions of academic instruction and possible variables in student evaluations. Positive correlations were observed between ratings of instruction quality and easiness, as well as between instruction quality and attractiveness. On average, professors in science and engineering disciplines have lower ratings than in the humanities and arts. When looking at {RateMyProfessors} as a whole, the effect of a professor’s gender on rating criteria is small but statistically significant. When analysing the data as a function of discipline, however, the effects of gender are significantly more pronounced, albeit more complex. The potential implications are discussed.},
	pages = {31--44},
	number = {1},
	journaltitle = {Assessment \& Evaluation in Higher Education},
	author = {Rosen, Andrew S.},
	urldate = {2023-09-22},
	date = {2018-01-02},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02602938.2016.1276155},
	keywords = {gender bias, online evaluations, {RateMyProfessors}, rating correlations, Student evaluations of teaching},
	file = {rosen2017.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\rosen2017.pdf:application/pdf},
}

@incollection{spiro_analysing_2016,
	location = {Cham},
	title = {Analysing {RateMyProfessors} Evaluations Across Institutions, Disciplines, and Cultures: The Tell-Tale Signs of a Good Professor},
	volume = {10046},
	isbn = {978-3-319-47879-1 978-3-319-47880-7},
	url = {http://link.springer.com/10.1007/978-3-319-47880-7_27},
	shorttitle = {Analysing {RateMyProfessors} Evaluations Across Institutions, Disciplines, and Cultures},
	abstract = {Can we tell a good professor from their students’ comments? And are there diﬀerences between what is considered to be a good professor by diﬀerent student groups? We use a large corpus of student evaluations collected from the {RateMyProfessors} website, covering diﬀerent institutions, disciplines, and cultures, and perform several comparative experiments and analyses aimed to answer these two questions. Our results indicate that (1) we can reliably classify good professors from poor professors with an accuracy of over 90\%, and (2) we can separate the evaluations made for good professors by diﬀerent groups with accuracies in the range of 71-89\%. Furthermore, a qualitative analysis performed using topic modeling highlights the aspects of interest for diﬀerent student groups.},
	pages = {438--453},
	booktitle = {Social Informatics},
	publisher = {Springer International Publishing},
	author = {Azab, Mahmoud and Mihalcea, Rada and Abernethy, Jacob},
	editor = {Spiro, Emma and Ahn, Yong-Yeol},
	urldate = {2023-09-22},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-47880-7_27},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Azab et al. - 2016 - Analysing RateMyProfessors Evaluations Across Inst.pdf:C\:\\Users\\Will\\Zotero\\storage\\37XC7GIT\\Azab et al. - 2016 - Analysing RateMyProfessors Evaluations Across Inst.pdf:application/pdf},
}

@article{scherr_single_2013,
	title = {Single comments or average ratings: which elements of {RateMyProfessors}.com™ shape university students’ judgments and course choice intentions?},
	volume = {25},
	issn = {1874-8600},
	url = {https://doi.org/10.1007/s11092-013-9164-z},
	doi = {10.1007/s11092-013-9164-z},
	shorttitle = {Single comments or average ratings},
	abstract = {The use and abuse of course and lecturer rating websites such as {RateMyProfessors}.com™ is a highly relevant topic for universities’ evaluation and assessment policies and practice. However, only a few studies have paid attention to the actual influence of teaching evaluation websites on the students themselves—that is, their perceptions of a certain course and their course choice intention at university. Findings point to the fact that positive comments on the website about professors improve students’ evaluations. However, professor evaluation websites contain two types of information: single student comments and average ratings. Research on exemplification effects has shown that single cases often have a stronger influence on recipients than more valid base rate information. We test this assumption in an experiment (n = 126) using a professor evaluation website stimulus. Results show that single comments strongly influence opinions and course choice intentions but that they are moderated by the valence of the average rating.},
	pages = {131--141},
	number = {2},
	journaltitle = {Educational Assessment, Evaluation and Accountability},
	shortjournal = {Educ Asse Eval Acc},
	author = {Scherr, Sebastian and Müller, Philipp and Fast, Victoria},
	urldate = {2023-09-22},
	date = {2013-05-01},
	langid = {english},
	keywords = {Exemplification effect, Experiment, Student behaviour/attitudes, Teaching evaluation website, University professor rating},
	file = {scherr2013.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\scherr2013.pdf:application/pdf},
}

@article{baker_quantcrit_2019,
	title = {A {QuantCrit} Approach: Using Critical Race Theory as a Means to Evaluate if Rate My Professor Assessments Are Racially Biased},
	volume = {3},
	rights = {Copyright (c)},
	issn = {2574-3481},
	url = {https://www.ojed.org/index.php/jump/article/view/1012},
	doi = {10.32674/jump.v3i1.1012},
	shorttitle = {A {QuantCrit} Approach},
	abstract = {Research on race is a paradigm in qualitative methodology. Researchers believe that when analyzing discrimination, numerical data may miss the subjective characteristics of bigotry. Since the early 1990s, research utilizing critical race theory in education employed a qualitative approach. Recent research using critical race theory includes a quantitative approach called {QuantCrit}. The online faculty evaluation site called Rate My Professor ({RMP}) is designed to allow students an opportunity to appraise faculty performance. Using evaluations of faculty in a Pennsylvania college from both {RMP} and {IOTA}360, this research examines the validity of {RMP} in analyzing minority faculty’s teaching. As predicted by applying a {QuantCrit} approach, results support that {RMP} evaluations show a race bias.},
	pages = {1--22},
	number = {1},
	journaltitle = {Journal of Underrepresented \& Minority Progress},
	author = {Baker, Chuck Alan},
	urldate = {2023-09-22},
	date = {2019-04-27},
	langid = {english},
	note = {Number: 1},
	keywords = {African American, Critical Race Theory, discrimination, faculty evaluations, minority, {QuantCrit}, racism},
	file = {Full Text PDF:C\:\\Users\\Will\\Zotero\\storage\\R9NLXINB\\Baker - 2019 - A QuantCrit Approach Using Critical Race Theory a.pdf:application/pdf},
}

@article{lewandowski_just_2012,
	title = {Just a harmless website?: an experimental examination of {RateMyProfessors}.com’s effect on student evaluations},
	volume = {37},
	issn = {0260-2938},
	url = {https://doi.org/10.1080/02602938.2011.594497},
	doi = {10.1080/02602938.2011.594497},
	shorttitle = {Just a harmless website?},
	abstract = {This set of experiments assessed the influence of {RateMyProfessors}.com profiles, and the perceived credibility of those profiles, on students’ evaluations of professors and retention of material. In Study 1, 302 undergraduates were randomly assigned to read positive or negative {RateMyProfessors}.com profiles with comments that focused on superficial or legitimate professor features. Participants then watched a lecture clip, provided professor ratings and completed a quiz on the lecture. In Study 2, 81 students who were randomly assigned to read the same {RateMyProfessors}.com profiles before an actual class provided credibility ratings of the information, listened to a lecture and provided professor ratings at the end. Across both experiments, one in a laboratory setting, and one in an authentic undergraduate lecture, participants gave more favourable professor ratings after reading positive evaluations from {RateMyProfessors}.com information. These findings establish the causal link between professor information and subsequent evaluations.},
	pages = {987--1002},
	number = {8},
	journaltitle = {Assessment \& Evaluation in Higher Education},
	author = {Lewandowski, Gary W. and Higgins, Emma and Nardone, Natalie N.},
	urldate = {2023-09-22},
	date = {2012-12-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02602938.2011.594497},
	keywords = {experiment, influences on evaluation, teaching evaluation, technology},
	file = {lewandowski2012.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\lewandowski2012.pdf:application/pdf},
}

@article{lee_rapport_2019,
	title = {Rapport, rigor, and rate my professor: students’ perceptions of hospitality and tourism professors},
	volume = {19},
	issn = {1531-3220},
	url = {https://doi.org/10.1080/15313220.2018.1509763},
	doi = {10.1080/15313220.2018.1509763},
	shorttitle = {Rapport, rigor, and rate my professor},
	abstract = {Student evaluation is a common measure used to assess the quality of an instructor’s teaching. However, the results of traditional student evaluation scores are often not available to students, thus students seek out other platforms to learn about professors that they have in mind as instructors for their future courses. Little is known about students’ evaluations of instructors on public, online teacher-rating websites, especially in the hospitality and tourism field. The purpose of this study was to explore hospitality and tourism students’ perceptions of instructors and their teaching effectiveness based on students’ comments and ratings. Results showed that hospitality and tourism students speak positively of their instructors and courses and rated them to be helpful, high in quality, and precise. The comments posted by students were also closely related to students’ perceptions on overall quality, helpfulness, clarity, and easiness.},
	pages = {93--111},
	number = {2},
	journaltitle = {Journal of Teaching in Travel \& Tourism},
	author = {Lee, Seung Hyun and Deale, Cynthia S.},
	urldate = {2023-09-22},
	date = {2019-04-03},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15313220.2018.1509763},
	keywords = {hospitality education, ratemyprofessor.com, Student evaluation, teaching effectiveness},
	file = {lee2018.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\lee2018.pdf:application/pdf},
}

@article{murray_does_2016,
	title = {Does {MTV} really do a good job of evaluating professors? An empirical test of the internet site {RateMyProfessors}.com},
	volume = {91},
	issn = {0883-2323},
	url = {https://doi.org/10.1080/08832323.2016.1140115},
	doi = {10.1080/08832323.2016.1140115},
	shorttitle = {Does {MTV} really do a good job of evaluating professors?},
	abstract = {Considerable debate continues regarding the efficacy of the website {RateMyProfessors}.com ({RMP}). To date, however, virtually no direct, experimental research has been reported which directly bears on questions relating to sampling adequacy or item adequacy in producing what favorable correlations have been reported. The authors compare the data provided by {RMP} relative to experimentally controlled, systematically collected data that offers a more rigorous evaluation of the findings reported by {RMP}. Six equivalent undergraduate course sections with 252 students were systematically studied with respect to professor evaluations using simple {RMP} scales versus a more complex, multi-item scale. Major findings were twofold: (a) {RMP} samples were entirely inadequate to statistically project the evaluations of other students at conventionally accepted research levels; and (b) statistically significant differences between stated {RMP} scores compared to those using a more ample attribute approach were found, with {RMP} evaluations biased in a negative direction.},
	pages = {138--147},
	number = {3},
	journaltitle = {Journal of Education for Business},
	author = {Murray, Keith B. and Zdravkovic, Srdan},
	urldate = {2023-09-22},
	date = {2016-04-02},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/08832323.2016.1140115},
	keywords = {student evaluation of teaching, online ratings, professor evaluations by students, professor ratings by students, {RateMyProfessors}.com, {SET}},
	file = {murray2016.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\murray2016.pdf:application/pdf},
}

@misc{tang_are_2021,
	title = {Are Top School Students More Critical of Their Professors? Mining Comments on {RateMyProfessor}.com},
	url = {http://arxiv.org/abs/2101.12339},
	doi = {10.48550/arXiv.2101.12339},
	shorttitle = {Are Top School Students More Critical of Their Professors?},
	abstract = {Student reviews and comments on {RateMyProfessor}.com reflect realistic learning experiences of students. Such information provides a large-scale data source to examine the teaching quality of the lecturers. In this paper, we propose an in-depth analysis of these comments. First, we partition our data into different comparison groups. Next, we perform exploratory data analysis to delve into the data. Furthermore, we employ Latent Dirichlet Allocation and sentiment analysis to extract topics and understand the sentiments associated with the comments. We uncover interesting insights about the characteristics of both college students and professors. Our study proves that student reviews and comments contain crucial information and can serve as essential references for enrollment in courses and universities.},
	number = {{arXiv}:2101.12339},
	publisher = {{arXiv}},
	author = {Tang, Ziqi and Wang, Yutong and Luo, Jiebo},
	urldate = {2023-09-22},
	date = {2021-01-23},
	eprinttype = {arxiv},
	eprint = {2101.12339 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Will\\Zotero\\storage\\H3TNTE7T\\Tang et al. - 2021 - Are Top School Students More Critical of Their Pro.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Will\\Zotero\\storage\\KV277MRW\\2101.html:text/html},
}

@article{boswell_ratemyprofessors_2016,
	title = {Ratemyprofessors is hogwash (but I care): Effects of Ratemyprofessors and university-administered teaching evaluations on professors},
	volume = {56},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563215302600},
	doi = {10.1016/j.chb.2015.11.045},
	shorttitle = {Ratemyprofessors is hogwash (but I care)},
	abstract = {This study investigated the effect of evaluation origin, Ratemyprofessors.com ({RMP}) or university-administered student evaluations of teaching ({UASET}), and valence, positive or negative, on professors' affect about teaching, self-efficacy for various aspects of teaching, and self-efficacy for professorial competence. Participants were 128 professors who responded to a recruitment email; they participated in an experimental manipulation via {SurveyMonkey} and were exposed to either {RMP} positive, {RMP} negative, {UASET} positive, or {UASET} negative evaluations. Evaluation valence alone influenced participants' affect and self-efficacy; affect and self-efficacy changed in the direction of feedback valence. Although participants consider {RMP} to be less accurate and less serious than {UASET}, {RMP} and {UASET} feedback influenced the participants equally. Feedback influence did not vary by participants' tenure status. Implications of {RMP} and {UASET} feedback on the professor–student relationship and student learning are discussed. Implications for {RMP} on professors and their employment are also discussed. Recommendations for the interpretation of {RMP} feedback are provided.},
	pages = {155--162},
	journaltitle = {Computers in Human Behavior},
	shortjournal = {Computers in Human Behavior},
	author = {Boswell, Stefanie S.},
	urldate = {2023-09-22},
	date = {2016-03-01},
	keywords = {Student evaluations of teaching, Ratemyprofessors.com, Self-efficacy},
	file = {boswell2016.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\boswell2016.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Will\\Zotero\\storage\\IX7Z7P37\\S0747563215302600.html:text/html},
}

@inproceedings{katrompas_rate_2021,
	title = {Rate My Professors: A Study Of Bias and Inaccuracies In Anonymous Self-Reporting},
	doi = {10.1109/CDS52072.2021.00098},
	shorttitle = {Rate My Professors},
	abstract = {Commentary-driven and opinion-driven “ratings” websites such as Yelp, Amazon Reviews, and Rate My Professors are ubiquitous on the Web. These review sites engage in voluntary self-reporting which is well known to be fraught with bias and inaccurate representations. This study is an investigation into the website known as Rate My Professors which ostensibly seeks to collect and report on college and university professor quality. Rate My Professors ({RMP}), like many review sites, is anonymous. While it is commonly believed that anonymity increases accuracy, studies have shown otherwise. Studies using anonymous self-reporting are well known to be unreliable, so much so that researchers are required to take compensatory measures to validate their results. Rate My Professors takes no such measures, and in fact there is no guarantee that a “student reviewer” is even a student. This study investigates Rate My Professors for bias, inaccuracy, and invalid data. The study will show compelling evidence which supports the idea that anonymous self-reporting, without compensatory validation measures, is flawed and unsuitable for use in a decision making process.},
	eventtitle = {2021 2nd International Conference on Computing and Data Science ({CDS})},
	pages = {536--542},
	booktitle = {2021 2nd International Conference on Computing and Data Science ({CDS})},
	author = {Katrompas, Alexander and Metsis, Vangelis},
	date = {2021-01},
	keywords = {Anonymous Reviews, Data science, Decision making, Online Ratings, Professor Evaluations, Rate My Professors, Ratings, Review Bias, Reviews, Self-Reporting, Self-Reporting Bias},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Will\\Zotero\\storage\\PLBSKFDU\\9463274.html:text/html;katrompasMetsis2021.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\katrompasMetsis2021.pdf:application/pdf},
}

@article{li_power_2013,
	title = {The power of {eWOM}: A re-examination of online student evaluations of their professors},
	volume = {29},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563213000101},
	doi = {10.1016/j.chb.2013.01.007},
	shorttitle = {The power of {eWOM}},
	abstract = {Over the past few years, a number of studies have examined the impact of the Rate My Professors ({RMP}) website ({RateMyProfessors}.com) on higher education. The focal area of examination is whether students’ evaluations of their professors on {RMP} are valid. The current study attempts to push the {RMP} discussions to a deeper level through two studies. Study 1 illustrates that students rely on the valence of review information on {RMP} to make their course selection decisions without considering its validity. Study 2 shows that students’ decision-making process tends to be biased as a result of review information available. The results of both studies suggest it is necessary for institutions to test new teaching evaluation models online.},
	pages = {1350--1357},
	number = {4},
	journaltitle = {Computers in Human Behavior},
	shortjournal = {Computers in Human Behavior},
	author = {Li, Cong and Wang, Xiuli},
	urldate = {2023-09-22},
	date = {2013-07-01},
	keywords = {{RateMyProfessors}.com, Ease-of-retrieval effect, Electronic word-of-mouth, {eWOM}, Online student evaluations, {RMP}},
	file = {li2013.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\li2013.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Will\\Zotero\\storage\\9XVXRQAB\\S0747563213000101.html:text/html},
}

@article{hartman_what_2013,
	title = {What Ratemyprofessors.com Reveals About How and Why Students Evaluate Their Professors: A Glimpse Into the Student Mind-Set},
	volume = {23},
	issn = {1052-8008},
	url = {https://doi.org/10.2753/MER1052-8008230204},
	doi = {10.2753/MER1052-8008230204},
	shorttitle = {What Ratemyprofessors.com Reveals About How and Why Students Evaluate Their Professors},
	abstract = {This study examines {RateMyProfessors}.com ratings and comments as a form of electronic word-of-mouth communications. The data represent 2,371 user ratings and comments for 442 marketing professors from 51 U. S. colleges and universities. Qualitative comments were analyzed using updated thematic content analyses. The results indicate significant inclusion of qualitative comments about professors' personal characteristics and teaching attributes. Consistent with the literature on word-of-mouth, social communications theory, and equity theory, more positive and negative comments were found than neutral or mixed remarks. Contrary to popular belief and findings in the literature on conventional word of mouth, comments overall were dominated by compliments rather than complaints.},
	pages = {151--162},
	number = {2},
	journaltitle = {Marketing Education Review},
	author = {Hartman, Katherine B. and Hunt, James B.},
	urldate = {2023-09-22},
	date = {2013-07-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.2753/{MER}1052-8008230204},
	file = {hartman2013.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\hartman2013.pdf:application/pdf},
}

@article{clayson_what_2014,
	title = {What does ratemyprofessors.com actually rate?},
	volume = {39},
	issn = {0260-2938},
	url = {https://doi.org/10.1080/02602938.2013.861384},
	doi = {10.1080/02602938.2013.861384},
	abstract = {This research looks closely at claims that ratemyprofessors.com creates a valid measure of teaching effectiveness because student responses are consistent with a learning model. While some evidence for this contention was found in three datasets taken from the site, the majority of the evidence indicates that the instrument is biassed by a halo effect, and creates what most accurately could be called a ‘likeability’ scale.},
	pages = {678--698},
	number = {6},
	journaltitle = {Assessment \& Evaluation in Higher Education},
	author = {Clayson, Dennis E.},
	urldate = {2023-09-22},
	date = {2014-08-18},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02602938.2013.861384},
	keywords = {construct validation, online evaluations, student evaluation of teaching},
	file = {clayson2013.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\clayson2013.pdf:application/pdf},
}

@article{davison_how_2009,
	title = {How do we rate? An evaluation of online student evaluations},
	volume = {34},
	issn = {0260-2938},
	url = {https://doi.org/10.1080/02602930801895695},
	doi = {10.1080/02602930801895695},
	shorttitle = {How do we rate?},
	abstract = {This paper analyses the popular {RateMyProfessors} ({RMP}) website where students evaluate instructors in higher education. A study was designed to measure (1) the awareness and utilisation of the {RMP} website, (2) the internal and external validity of the {RMP} ratings in measuring teaching effectiveness, and (3) variation in the above across disciplines. It is concluded that the category of ratings, created by the website, establishes an anti‐intellectual tone that manifests itself in comments about instructors’ personality, easiness of workload and entertainment value rather than knowledge attained.},
	pages = {51--65},
	number = {1},
	journaltitle = {Assessment \& Evaluation in Higher Education},
	author = {Davison, Elizabeth and Price, Jammie},
	urldate = {2023-10-07},
	date = {2009-02-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02602930801895695},
	keywords = {online ratings, Rate My Professors, student evaluations, teaching effectiveness, teaching evaluations},
	file = {Submitted Version:C\:\\Users\\Will\\Zotero\\storage\\NM4BJHGR\\Davison and Price - 2009 - How do we rate An evaluation of online student ev.pdf:application/pdf},
}

@article{kowai-bell_rate_2011,
	title = {Rate My Expectations: How online evaluations of professors impact students' perceived control},
	volume = {27},
	doi = {10.1016/j.chb.2011.04.009},
	shorttitle = {Rate My Expectations},
	pages = {1862--1867},
	journaltitle = {Computers in Human Behavior},
	shortjournal = {Computers in Human Behavior},
	author = {Kowai-Bell, Neneh and Guadagno, Rosanna and Chase, Tannah and Preiss, Najean and Hensley, Rachel},
	date = {2011-09-01},
	file = {kowai-bell2011.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\kowai-bell2011.pdf:application/pdf},
}

@article{chiang_students_2017,
	title = {Students' perspectives on {RateMyProfessors}.com: an empirical investigation of perception and attitude},
	volume = {5},
	issn = {2050-3954},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJSMILE.2017.086083},
	doi = {10.1504/IJSMILE.2017.086083},
	shorttitle = {Students' perspectives on {RateMyProfessors}.com},
	abstract = {Since its inception in 1999, the website of {RateMyProfessors}.com has been popular and has gathered more than 15 million ratings of approximate 1.5 million professors from more than 7,000 schools. It provides college students opportunities to rate and comment on their instructors and helps them with course selection. The purpose of this study was to investigate the college student perception and attitude towards this website. We analysed survey data from 166 undergraduate students. Our findings suggest that overall students have positive experiences and find {RMP} as a reliable source of information for course selection. About 25\% of participants who have posted ratings have stronger positive attitude towards the website and higher tendency to check the website before registering classes.},
	pages = {21--31},
	number = {1},
	journaltitle = {International Journal of Social Media and Interactive Learning Environments},
	author = {Chiang, Kuan-Pin},
	urldate = {2023-10-07},
	date = {2017-01},
	note = {Publisher: Inderscience Publishers},
	keywords = {attitude, faculty evaluations, faculty ratings, perception, {RateMyProfessors}, student evaluations},
	file = {Chiang - 2017 - Students' perspectives on RateMyProfessors.com an.pdf:C\:\\Users\\Will\\Zotero\\storage\\KHLXMUGJ\\Chiang - 2017 - Students' perspectives on RateMyProfessors.com an.pdf:application/pdf},
}

@article{bleske-rechek_ratemyprofessors_2010,
	title = {{RateMyProfessors}  com: Testing Assumptions about Student Use and Misuse},
	volume = {15},
	issn = {1531-7714},
	url = {https://scholarworks.umass.edu/pare/vol15/iss1/5},
	doi = {https://doi.org/10.7275/ax6d-qa78},
	shorttitle = {{RateMyProfessors}  com},
	number = {1},
	journaltitle = {Practical Assessment, Research, and Evaluation},
	author = {Bleske-Rechek, April and Michels, Kelsey},
	date = {2010},
	file = {"RateMyProfessors com\: Testing Assumptions about Student Use and Misus" by April Bleske-Rechek and Kelsey Michels:C\:\\Users\\Will\\Zotero\\storage\\USSXC7RT\\5.html:text/html;Bleske-Rechek2010.pdf:C\:\\Users\\Will\\OneDrive - Northern Kentucky University\\NKU\\G.    Fall 2023\\HNR 497\\papers\\Bleske-Rechek2010.pdf:application/pdf},
}

@article{boswell_effects_2020,
	title = {Effects of Ratemyprofessors.com and University Student Evaluations of Teaching on Students’ Course Decision-Making and Self-Efficacy},
	volume = {10},
	issn = {2157-6254},
	url = {https://scholarworks.waldenu.edu/hlrc/vol10/iss2/9},
	doi = {10.18870/hlrc.v10i2.1194},
	abstract = {This study investigated effects of Ratemyprofessors.com and university student evaluations of teaching on students’ course decision-making and self-efficacy in an ethnically diverse undergraduate sample. It also investigated if these effects were impacted by evaluation positivity. Additionally, the study explored if attitudes toward Ratemyprofessors.com were related to student gender, college class, and age. Participants were 73 undergraduates who were exposed to positive and negative evaluations about fictitious professors; participants were informed that the evaluations originated from Ratemyprofessors.com or university student evaluations of teaching. Evaluation positivity but not type influenced students’ intention to enroll in the professor’s course, but not how seriously they would consider the feedback. Evaluation positivity also influenced self-efficacy. Beliefs about and use of Ratemyprofessors.com were not related to student gender, college class, or age. Evaluation positivity’s effect on student course decision making and self-efficacy has implications for for university students, faculty, and administrators.},
	number = {2},
	journaltitle = {Higher Learning Research Communications},
	shortjournal = {hlrc},
	author = {Boswell, Stefanie S.},
	urldate = {2023-10-07},
	date = {2020-09-01},
	langid = {english},
	file = {Boswell - 2020 - Effects of Ratemyprofessors.com and University Stu.pdf:C\:\\Users\\Will\\Zotero\\storage\\GGCV4UW4\\Boswell - 2020 - Effects of Ratemyprofessors.com and University Stu.pdf:application/pdf},
}

@article{hayes_student_2014,
	title = {Student use of quantitative and qualitative information on {RateMyProfessors}.com for course selection},
	volume = {48},
	issn = {01463934},
	url = {https://go.gale.com/ps/i.do?p=AONE&sw=w&issn=01463934&v=2.1&it=r&id=GALE%7CA398073357&sid=googleScholar&linkaccess=abs},
	abstract = {{\textless}em{\textgreater}Gale{\textless}/em{\textgreater} Academic {OneFile} includes Student use of quantitative and qualitative information by Matthew W. Hayes and Joseph Prus. Click to explore.},
	pages = {675--689},
	number = {4},
	journaltitle = {College Student Journal},
	author = {Hayes, Matthew W. and Prus, Joseph},
	urldate = {2023-10-08},
	date = {2014-12-01},
	note = {Publisher: Project Innovation Austin {LLC}},
	file = {Snapshot:C\:\\Users\\Will\\Zotero\\storage\\IHY4JYDS\\i.html:text/html},
}

@article{johnson_i_2014,
	title = {I Feel Like Shooting Myself in the Face after taking this God-forsaken Class: The Effects of {RateMyProfessors}.com on University Course Registration},
	volume = {4},
	shorttitle = {I Feel Like Shooting Myself in the Face after taking this God-forsaken Class},
	abstract = {This study examines the effects of {RateMyProfessors}.com professor ratings on student decisions related to the Fall 2012 course registration period. Statistical techniques including ordinary least squares regression and survival analysis are employed to model course registration. We operate within an applied economic framework in which the supply and demand for course section seating capacity do not fully equalize due to various physical constraints as well as the existence of imperfect information. By further understanding the factors underpinning student decision making, university departmental management is better able to estimate changes in student demand. {RateMyProfessors}.com reported measures of overall quality, easiness, and attractiveness are determined to be positively related, in varying magnitude, to course enrollment. {JEL} classification numbers: I20},
	pages = {31--42},
	journaltitle = {Advances in Management and Applied Economics},
	shortjournal = {Advances in Management and Applied Economics},
	author = {Johnson, Jeremiah and Hoover, David and Beck, Jason and Toma, Michael},
	date = {2014-01-01},
	file = {Full Text PDF:C\:\\Users\\Will\\Zotero\\storage\\Y7BRUQY2\\Johnson et al. - 2014 - I Feel Like Shooting Myself in the Face after taki.pdf:application/pdf},
}
